{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"welcome to my digital lab notebook","text":""},{"location":"#about-me","title":"about me","text":"<p>I'm an entrepreneur-engineer, researcher, musician, and linguaphile. I am co-founder and CTO at Olibra (Bond Home), where I lead a team building next-generation user experiences for smart home. I've done research in the past in causal inference (think of it as the less trendy little sister of AI Interpretability), and insofar as I have research interests at the moment, I am toying with:</p> <ul> <li>technical approaches to AI safety, especially via interpretability</li> <li>how character-level transformer models may learn morphemes and their meanings</li> <li>training through self-play, in games and mathematics</li> </ul> <p>Otherwise I spend my time doodling on guitar, taking friends foraging, ever so gradually learning Latin, and teaching kids to solder and code.</p> <p>My intent with this site is to share occasional writeups related to work, research, and adjacent. Try the sidebar menu.</p>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to my blog, where I share insights and solutions for embedded software development challenges.</p>"},{"location":"blog/2024/05/05/crashes-in-the-field/","title":"Crashes in the Field","text":"<p>As developers, we should write software that doesn't crash. Crashes are, at best, warts on the user experience. Nobody likes warts, do they?</p> <p>If we cannot catch every way the system can crash while developing, we've got to have some means of pulling crash data back from the software that we deploy.</p> <p>My goal here is to demystify the </p>"},{"location":"blog/2024/05/05/crashes-in-the-field/#desiderata","title":"Desiderata","text":"<p>Our minimalist embedded observability solution should be:</p> <ul> <li>efficient</li> <li>portable</li> <li>accurate</li> <li>maintainable</li> <li>and usable</li> </ul>"},{"location":"embedded/","title":"Blog","text":""},{"location":"research/charformers/","title":"Character-based Transformers","text":"<p>Way back when I had fun with little n-gram models, that I fondly recall could write pseudo-Tolkien when training it on a text copy of The Hobbit. It was fun to watch how as n increased from 3 to 5 the words became more and more coherently spelled. You now understand my sadness at the tokenization step used in modern LLMs: it strips them of the  very first learning step of discovering the morphemes, smallest units of meaning.</p> <p>So I'd like to experiment with some character-based transformers. I have no expectation that I'd be able to get coherent English text out of them without massive GPU resources, but perhaps we can intepret some patterns of morphological learning?</p>"},{"location":"research/charformers/#sanitizing-the-data","title":"Sanitizing the Data","text":"<p>We are concerned with English text, and we want to keep  the language as small as possible, so let's just work with lowercase letters plus space and a few punctuation characters:</p> <p><code>abcdefghijklmnopqrstuvwxyz .,?'\\n</code></p> <p>Note that we've merged the apostrophe, single, and double quotes, we keep distinct period (but convert exclaimation to period), a distinct question mark, and all other punctuation will be replaced with spaces. Newline has its own symbol, which represents a paragraph break or could be used as a stop token.</p> <p>For my purposes I'll feed in a text copy of The Lord of the Rings, because I know this book so well that it should help me interpret the small model gibberish I'm sure to get when sampling generatively.</p>"},{"location":"research/charformers/#n-grams-revisited","title":"N-Grams Revisited","text":"<p>I like to start with the simplest possible model, which within the neural network paradigm would be a single-layer network. (1)</p> <ol> <li>If we want to be pedantic, it would be a zero-layer network with just a bias term predicting just based on character frequency.</li> </ol>"},{"location":"research/threedigit-notes/","title":"Three Digit Math","text":"<p>I devoured Heinlein's Stranger in a Strange Land in high school, so naturally I was excited to hear about neural nets \"grokking\" three digit addition. I recall seeing a graph of training and test accuracy over time, where the training accuracy reaches 100% early on, but with test accuracy at chance---no generalization---but then after a long period of stasis, the test accuracy quite suddenly shoots up to 100% also. The network \"grokked\" the problem.</p> <p>So I ask, what's the simplest model I can build that can grok three digit addition? And, can we look inside and see how it does it?</p> <p>Now, I could go and read the paper, but first I'm just going to play around myself.</p> <p>\"Research is what I'm doing when I don't know what I'm doing.\" - Wernher von Braun</p>"},{"location":"research/threedigit-notes/#dataset","title":"Dataset","text":"<p>Our dataset will be all 3-digit addition equations that have 3-digit answers, such as</p> \\[ 123 + 456 = 579. \\] <p>I'll simply enumerate all the equations:</p> <pre><code>def all_equations():\n    for i in range(100, 1000):\n        for j in range(100, 1000):\n            k = i + j\n            if k &gt; 999:\n                break\n            yield f\"{i}+{j}={k}\"\n</code></pre> <p>then shuffle, and split into 90% training and 10% test.</p>"},{"location":"research/threedigit-notes/#embeddings","title":"Embeddings","text":"<p>Now, we need to decide how to encode the digits as vectors. The simplest approach is to use one-hot encoding, where each digit is represented as a vector of length 10 with just a single dimension set to 1, the rest set to 0. So our NN will be learning a mapping:</p> \\[ \\mathbb{R}^{6 \\times 10} \\rightarrow \\mathbb{R}^{3 \\times 10} \\] <p>For the example above, the input is</p> \\[ X = \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix}, \\] <p>and the correct output is</p> \\[ Y = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}. \\] <p>To the network, the input is a 60-dimensional vector, I've just formatted it as a 6x10 matrix for clarity.</p>"},{"location":"research/threedigit-notes/#a-first-dumb-model","title":"A First Dumb Model","text":"<p>Let's start with the simplest possible model, one without any hidden layers:</p> <pre><code>graph LR\n    subgraph Inputs\n        I[6 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;60 values]\n    end\n\n    subgraph Output Layer\n        O[3 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;30 neurons]\n    end\n\n    I --&gt; O</code></pre>"},{"location":"research/threedigit-notes/#the-math","title":"the math","text":"<p>Let's look at the math for this model. We have a linear transformation of the input:</p> \\[ Z = W X + b \\] <p>where:</p> <ul> <li>\\(X \\in \\mathbb{R}^{60}\\) is the input vector</li> <li>\\(W \\in \\mathbb{R}^{30 \\times 60}\\) is the weight matrix</li> <li>\\(b \\in \\mathbb{R}^{30}\\) is the bias vector</li> <li>\\(Z \\in \\mathbb{R}^{30}\\) is the pre-activation output</li> </ul> <p>Then applying softmax to get the final output for each digit position \\(d \\in \\{0, 1, 2\\}\\) and each digit class \\(c \\in \\{0, 1, ..., 9\\}\\):</p> \\[ Y_{d,c} = \\frac{e^{Z_{d,c}}}{\\sum_{c'=0}^{9} e^{Z_{d,c'}}} \\] <p>This gives us probability distributions over the 10 possible values for each of the 3 output digits.</p> <p>Our little \"simplest model\" already has 1830 parameters!</p>"},{"location":"research/threedigit-notes/#lets-fire-up-an-autograd","title":"let's fire up an autograd?","text":"<p>To be continued...</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/embedded/","title":"embedded","text":""}]}