{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"welcome to my digital lab notebook","text":""},{"location":"#about-me","title":"about me","text":"<p>I'm an entrepreneur-engineer, erstwhile researcher, amateur musician and radio operator (W2CAM), and linguaphile. I am co-founder and CTO at Olibra (Bond Home), where I lead a team building next-generation user experiences for smart home. I've done research in causal inference.</p> <p>I have become deeply concerned about dangers of machine intelligence, and my blog here represents my attempt to get up to speed on technical AI safety research and perhaps make a contribution to this going well.</p> <p>Research directions I'm particularly interested in:</p> <ul> <li>automated discovery and explaination of reasoning failures</li> <li>evaluations of agentic coding against real machines (qemu, attached hardware)</li> <li>byte-based transformers (direct machine code generation)</li> </ul>"},{"location":"#current-project","title":"current project","text":"<p>I'm currently enrolled in the Frontier Language Models Fractal U course, and I'm blogging my course projects. Currently studying reasoning failures in DeepSeek models.</p>"},{"location":"#past-projects","title":"past projects","text":"<p>I'm curious about AI interpretability, so I reproduced the intro figure  from Anthropic's Toy Models of Superposition. I wanted a refresher, so rather than use an autograd I worked out the gradients by hand and implemented in C. My small contribution is animating the embedding during learning which gives a physical intuition about local minima.</p> <p>I've been dabbling with NLP/ML for a long while. Here's a blog post from 2011 on KNN visualization that shows how I can make fully custom figures to communicate mathematical ideas.</p> <p>I once came up with a backdoor way to align an audio book with the text: use even poor quality TTS software that gives timestamps and then use DTW algo to to align the two audio files.</p> <p></p> <p>At my smart home company, we've watched as each successive LLM opens new home control capabilities.  Together with a front-end engineer, I built a home control demo the morning that <code>gpt-4o</code> was released. The faster inference dropped the latency to the point it made for a compelling demo. I had originally built this on davinci-003 before tool calling was available, and I still used nearly the same prompt with lots of JSON examples. (1) </p> <ol> <li>I still think it would be amazing to be able to specify a grammar for the LLM to follow. Should be easy to just condition the output probabilities on being grammatical (provided there's no garden paths :hmm:).</li> </ol> <p>Lastly, I also do the mundane stuff that has to happen around the research. Here's a figure I drew for one of Kleinberg Lab's papers in grad school. I wrote the video annotation software and fabricated a custom table in a woodshop in addition to coding the ML pipeline.</p> <p></p>"},{"location":"#selected-papers","title":"selected papers","text":"<ul> <li> <p>Causal Explaination, AAAI (2016) - Computing the strength of causal relationships between events in a physical model.</p> </li> <li> <p>Physical Model Theory (2010) - A broad exploration of the simplest world-models used in physics.</p> </li> <li> <p>Fuzzy Flip-Flops (2010) - Relaxes the binary nature of logic gates to explore continuous-valued logic.</p> </li> <li> <p>Finite Machines (2009) - A formal exploration of small machines with limited memory.</p> </li> <li> <p>Sunglint Statistics, Applied Optics (2008) - We study the pattern of light reflected from the ocean surface in order to help remove it from satellite imagery. I'm most proud of the attractor diagrams that reveal structure in the glint time series.</p> </li> </ul>"},{"location":"#misc","title":"misc","text":"<p>Otherwise I spend my time doodling on guitar, taking friends foraging, ever so gradually learning Latin, and teaching kids to solder and code. My intent with this site is to share occasional writeups related to work, research, and adjacent. Try the sidebar menu.</p>"},{"location":"blog/","title":"Merck's Blog","text":""},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/","title":"Reasoning Failures in DeepSeek V3 Base","text":"<p>Wherein we observe arithmatic errors, infinite loops, handwaving, and wishful thinking. -- Coursework for Frontier Language Models (Summer 2025, Fractal U).</p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#intro","title":"Intro","text":"<p>In this post, I manually analyze specific reasoning failures of DeepSeek V3 Base (1). Why the base model? Because if we are to understand how reasoning emerges during RL, we should start by studying the nacent reasoning capabilities of the base models that are the starting point for the likes of R1 (2) and presumably o3, and how failures are avoided. The next post will explore  reasoning failures in the R1 model.</p> <p>We will investigate a simple verifiable math problem for which the base model solves ~75% of the time, and which R1 always solves. This way we can see some of the failures which GRPO RL is able to optimize away.</p> <ol> <li>DeepSeek V3 is a  671B parameter Mixture-of-Experts model released in December 2024.</li> <li>Just a month after announcing the base model,  DeepSeek published a reasoning model DeepSeek R1 competative with then state-of-the-art OpenAI o1. R1 was trained by mostly RL against a rule-based reward function of math problems with known solutions and coding challenges  that can be checked by compilation and unit tests.</li> </ol>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#method","title":"Method","text":"<p>For the purposes of this post I use a math problem of a kind likely used (1) to train R1, a 3x3 linear system of equations:</p> <ol> <li>We do not know exactly which math problems with deterministic results were used by the DeepSeek team.</li> </ol> \\[ \\left\\{ \\begin{aligned} 4x - 3y + 6z &amp;= 6 \\\\ -3x + 4y + 5z &amp;= 5 \\\\ 3x + 9y - 7z &amp;= -7 \\end{aligned} \\right. \\] <p>This problem is hard enough that we observe frequent and varied failures in V3 Base, but easy enough that perfect performance is achieved by R1.</p> <p>I use the exact prompt template given in the DeepSeek R1 paper:</p> <p>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <code>&lt;think&gt;</code> <code>&lt;/think&gt;</code> and <code>&lt;answer&gt;</code> <code>&lt;/answer&gt;</code> tags, respectively, i.e., <code>&lt;think&gt;</code> reasoning process here <code>&lt;/think&gt;</code> <code>&lt;answer&gt;</code> answer here <code>&lt;/answer&gt;</code>. User: <code>{problem}</code>. Assistant:</p> <p>And then in this post I'm using just this <code>problem</code>:</p> <p>Solve the system of equations:  \\(4x -3y +6z = 6\\),  \\(-3x +4y +5z = 5\\),  \\(3x +9y -7z = -7\\). Give your solution as a JSON array of integers.</p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#failures","title":"Failures","text":"<p>I ran the above prompt a number of times and sampled several of the failure cases below with commentary.</p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#handwaving","title":"Handwaving","text":"<p>Sometimes V3 Base will describe how to solve the problem rather than actually solving it. I hypothesize that in the handwaving example below, the failure is mediated by the use of the auxiliary  verbs can and will. </p> <p>To solve the system of equations, we can use the method of substitution or elimination. Here, we will use the method of elimination. First, we will eliminate the variable x from the first and second equations. To do this, we will multiply the first equation by 3 and the second equation by 4, and then add the two equations together. This will give us a new equation with only y and z. Next, we will eliminate the variable x from the first and third equations. To do this, we will multiply the first equation by 3 and the third equation by 4, and then subtract the two equations. This will give us another new equation with only y and z. Now, we have two equations with only y and z. We  can solve these two equations simultaneously to find the values of y and z. Once we have the values of y and z, we can substitute them into any of the original equations to find the value of x. Finally, we will have the values of x, y, and z, which we can represent as a JSON array [x, y, z].</p> <p>This would be a great preliminary plan, but the model did not actually work through the computation steps, and so it then gives the final answer:</p> <p><code>[1, 2, 3]</code></p> <p>This looks like an example rather than a guess. This is akin to behavior I've seen from OpenAI models where rather than returing a functional program safisying my request, they return an \"example\" of how I could do it. </p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#arithmetic-errors-infinite-loops","title":"Arithmetic Errors &amp; Infinite Loops","text":"<p>It is well-known that base models tend to end up in repeating patterns. In the completion below, we see such a repeating sequence emerge,  but looking closely we can see that an arithmetic error occurred prior: in simplifying \\(\\frac{45}{4}\\frac{38}{7}\\), the model eliminated the factor of 2 from the nominator (yielding \\(45 * 38 / 2 = 855\\)) but then did not eliminate the factor from the denominator (\\(4 * 7 = 28\\) rather than \\(4 * 7 / 2 = 14\\)).</p> <p><code>&lt;think&gt;</code> To solve the system of equations, we can use the method of elimination or substitution. However, since the coefficients of the variables are not the same in all equations, it might be easier to use the method of substitution. First, we can solve the first equation for x: \\(4x = 3y - 6z + 6\\) \\(x = \\frac{3y - 6z + 6}{4}\\) Next, we can substitute this expression for x into the second and third equations: \\(-3(\\frac{3y - 6z + 6}{4}) + 4y + 5z = 5\\) \\(3(\\frac{3y - 6z + 6}{4}) + 9y - 7z = -7\\) Simplifying these equations, we get: \\(-\\frac{9}{4}y + \\frac{9}{2}z - \\frac{9}{2} + 4y + 5z = 5\\) \\(\\frac{9}{4}y - \\frac{9}{2}z + \\frac{9}{2} + 9y - 7z = -7\\) Combining like terms, we get: \\(\\frac{7}{4}y + \\frac{19}{2}z = \\frac{19}{2}\\) \\(\\frac{45}{4}y - \\frac{23}{2}z = -\\frac{23}{2}\\) Now, we can solve the first equation for y: \\(\\frac{7}{4}y = \\frac{19}{2} - \\frac{19}{2}z\\) \\(y = \\frac{38}{7} - \\frac{38}{7}z\\) Substituting this expression for y into the second equation, we get: \\(\\frac{45}{4}(\\frac{38}{7} - \\frac{38}{7}z) - \\frac{23}{2}z = -\\frac{23}{2}\\) Simplifying this equation, we get: \\(\\frac{855}{\\color{red}{28}} - \\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2}\\) Combining like terms, we get: \\(-\\frac {855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}\\) \\(\\color{red}{-\\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}}\\) \\(-\\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}\\) \\(-\\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}\\) \\(-\\frac{855}{28}z - \\frac{23}{2 }z = -\\frac{23}{2} - \\frac{855}{28}\\) \\(-\\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}\\) \\(-\\frac{855}{28}z - \\frac{23}{2}z = -\\frac{23}{2} - \\frac{855}{28}\\) </p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#wishful-thinking","title":"Wishful Thinking","text":"<p>Lastly, I see a case of what I'll call wishful thinking. V3 Base suceeds at eliminating \\(x\\) in the first step, but then it tries to eliminate \\(y\\) in the same way, neglecting that a multiplication of one equation by \\(6\\)  is required in order to achieve elimination.</p> <p>Presumably the model at this point had a choice whether to actually do the correct computation, leading to an unintended result where \\(y\\) was not in fact eliminated, or to copy the pattern from the first step even though the computation is wrong. It chooses the latter, which though of course reduces to  statitical pattern matching, has the appearance of wishful thinking.</p> <p><code>&lt;think&gt;</code> To solve the system of equations, we can use the method of elimination or substitution. However, since the equations are already in a form that allows for easy elimination, we will use that method. First, we will eliminate \\(x\\) from the second and third equations by adding the first equation to the second and third equations. This gives us the system of equations: \\(4x -3y +6z = 6\\), \\(y +11z = 11\\), \\(6y -z = -1\\). Next, we will eliminate \\(y\\) from the third equation by adding the second equation to the third equation. This gives us the system of equations: \\(4x -3y +6z = 6\\), \\(y +11z = 11\\), \\(\\color{red}{10z = 10}\\). Finally, we can solve for \\(z\\) by dividing both sides of the third equation by 10. This gives us \\(z = 1\\). Substituting this value of \\(z\\) into the second equation, we can solve for \\(y\\). This gives us \\(y = 0\\). Substituting these values of \\(y\\) and \\(z\\) into the first equation, we can solve for \\(x\\). This gives us \\(x = 1\\). Therefore, the solution to the system of equations is \\(x = 1\\), \\(y = 0\\), \\(z = 1\\). <code>&lt;/think&gt;</code> <code>&lt;answer&gt;</code> <code>[1, 0, 1]</code></p>"},{"location":"blog/2025/06/22/reasoning-failures-in-deepseek-v3-base/#next-steps","title":"Next Steps","text":"<p>I see two directions to take this exploration:</p> <ol> <li> <p>Explore reasoning failures in a reasoning model (R1): does it make the same mistakes only less often? Or does it fail in new ways? This will require harder math problems.</p> </li> <li> <p>Automate the discovery of the location of the reasoning failure in the chain of thought. One way to do this could be to look at the probability of success as a function of position in a particulary faulty output. I expect that, at least in the case of arithmatic errors, we could automatically discover the exact token at which the error occured, but this does require a lot of inference. I'd like to explore statictical and heuristic methods for efficiently deriving failure explainations.</p> </li> </ol>"},{"location":"blog/2025/05/14/demonstrating-superposition/","title":"Demonstrating Superposition","text":"<p>Wherein I implement a toy model of feature superposition by hand in C as a remedial exercise, and create a video showing the model learning a suboptimal representation.</p> <p>In the unlikely scenario where all of this makes total sense and you feel like you're ready to make contributions, [...] - Scott Alexander Nov 2023</p> <p>In this post, I will manually reproduce the intro figure from Toy Models of Superposition without using anything but the C standard library, so as not to hide any of the details. The paper comes with a PyTorch implementation, but autograds do so much work I feel I need to earn the right to use them by working out the toy model math and code myself.</p> <p>The basic result is this little animation showing how the model learns the pentagonal representation from the paper's intro:</p>    Your browser does not support the video tag."},{"location":"blog/2025/05/14/demonstrating-superposition/#the-data","title":"The Data","text":"<p>First off, we need to generate the synthetic data. We want samples with dimension \\(n=5\\), where features are sparse (being non-zero with probability \\(1 - S\\)) and uniformly  distributed on the unit interval when they do appear, which we can write down as a  mixture of two distributions:</p> \\[x_i \\sim \\sum \\begin{cases} \\delta(0) &amp; S \\\\ \\text{U}(0, 1) &amp; (1 - S) \\end{cases}\\] <p>Here \\(\\delta\\) is the Dirac delta function, i.e. the point mass distribution.</p>"},{"location":"blog/2025/05/14/demonstrating-superposition/#synthesizing-data-in-c","title":"Synthesizing Data in C","text":"<p>The C stdlib doesn't have a uniform random function so I wrote one (1) and used it to generate the data:</p> <ol> <li><pre><code>#include &lt;stdlib.h&gt;\n\n/// get a float uniformly distributed on U[0, 1)\nfloat frand() {\n    return (random() / (float) RAND_MAX);\n}\n</code></pre> macOS manpages implore us to use the cryptographically secure RNG <code>arc4random()</code>,  but I think the polynomial PRNG is good enough for this application,  and I like that we can use <code>srandom(0)</code> to force reproducibility.</li> </ol> <pre><code>void synthesize(int n, long count, float X[n][count], float S_) {\n    // sparsity S in [0, 1), S_ is 1-S\n    for (long c = 0; c &lt; count; c++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (frand() &lt; S_) {\n                X[i][c] = frand();\n            }\n        }\n    }\n}\n</code></pre> <p>Now we can generate some samples with sparsity 1-S = 0.1,  using a little <code>printmat</code> function (1) to check our work. Below we see the result for four 5-D samples.</p> <ol> <li><pre><code>void printmat(char * tag, int rows, int cols, float A[rows][cols]) {\n    printf(\"%s: [\\n\", tag);\n    for (int m = 0; m &lt; rows; m++) {\n        for (int n = 0; n &lt; cols; n++) {\n            if (A[m][n]) {\n                printf(\"  %1.03f \", A[m][n]);\n            } else {\n                printf(\"  0     \");\n            }\n        }\n        printf(\"\\n\");\n    }\n    printf(\"]\\n\");\n}\n\nconst int count = 4;\nsrandom(0);\nmemset(X, 0, sizeof(X));\nsynthesize((float *) X, count, 0.1);\nprintmat(\"X\", (float *) X, N, count);\n</code></pre></li> </ol> \\[ X=\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.522 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.568 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] <p>Here we see that only ~2 of the 20 elements are non-zero, as expected with this sparsity level.</p>"},{"location":"blog/2025/05/14/demonstrating-superposition/#the-model","title":"The Model","text":"<p>The model is a 2-layer feedforward network, where the hidden layer maps down from \\(n=5\\) to \\(m=2\\) dimensions without any activation function, and then the output layer uses the transpose of the hidden-layer weights plus a bias term and a ReLU activation function.  This is, as far as I can tell, basically an autoencoder.</p> <p>In matrix notation we have:</p> \\[y = \\verb|ReLU|(W^T W x + b).\\]"},{"location":"blog/2025/05/14/demonstrating-superposition/#the-forward-pass","title":"The Forward Pass","text":"<p>Breaking down into steps with indecies we have:</p> \\[ \\begin{aligned} h_k &amp;= \\sum_{i=1}^n w_{ki} x_i \\\\ a_j &amp;= b_j + \\sum_{k=1}^m h_k w_{kj} \\\\ y_j &amp;= \\max(0, a_j), \\end{aligned} \\] <p>from which follows a first C implementation of the forward pass:</p> <pre><code>void forward(params_t * p, float x[N], float * y) {\n    float hk[M];\n    memset(hk, 0, sizeof(hk));\n    // hidden layer\n    for (int k = 0; k &lt; M; k++) {\n        for (int i = 0; i &lt; N; i++) {\n            hk[k] += p-&gt;W[k][i] * x[i];\n        }\n    }\n    // output layer\n    for (int j = 0; j &lt; N; j++) {\n        y[j] += p-&gt;b[j];\n        for (int k = 0; k &lt; M; k++) {\n            y[j] += p-&gt;W[k][j] * hk[k];\n        }\n        // ReLU activation\n        y[j] = y[j] &gt; 0 ? y[j] : 0;\n    }\n}\n</code></pre>"},{"location":"blog/2025/05/14/demonstrating-superposition/#importance-and-loss","title":"Importance and Loss","text":"<p>We want the model to prioritize representation of certain dimensions, so we assign an importance \\(I_i\\) to each dimension, which we  make decrease geometrically: \\(I_i = 0.7^i\\). A weighted least-squares loss is then:</p> \\[L = \\frac{1}{2} \\sum_i I_i (y_i - x_i)^2 + \\alpha \\sum_{k,j} w_{kj}^2.\\] <p>And our goal is to optimize the parameters \\(W\\) and \\(b\\) to minimize this loss. We then should be able to visualize the weights and see feature superposition emerging as a function of sparsity.</p> <p>Note that in the paper they do not specify any regularization. I threw in the L2 regularization term because I saw that a weight-decay  optimizer was used in the paper's code example on CoLab, but it turns out to be totally unnecessary if we pick the learning rate right.</p>"},{"location":"blog/2025/05/14/demonstrating-superposition/#training","title":"Training","text":""},{"location":"blog/2025/05/14/demonstrating-superposition/#finding-the-gradient","title":"Finding the Gradient","text":"<p>As I'm a bit rusty on my calculus, I'll go step by step through the gradient computation. Taking the derivative with respect to an arbitrary weight and pushing the derivative inside the sums as far as it will go, applying the chain and power rules,  and using \\(\\delta_j\\) to denote the error in the \\(j\\)th output, we have:</p> \\[ \\begin{align*} \\frac{\\partial L}{\\partial w_{kj}} &amp;= \\frac{1}{2} \\sum_i I_i \\frac{\\partial}{\\partial w_{kj}}       \\bigl(y_i - x_i\\bigr)^2   + \\alpha \\sum_{k}\\sum_{j'} \\frac{\\partial}{\\partial w_{kj}} w_{kj'}^2, \\\\[1ex] &amp;= \\sum_i I_i\\,\\delta_i\\,\\frac{\\partial y_i}{\\partial w_{kj}}   + \\alpha\\,w_{kj}. \\end{align*} \\] <p>Note that in the regularization term we've used the fact that the only summand that depends on \\(w_{kj}\\) is the one where \\(k' = k\\) and \\(j' = j\\), so the primes drop off the indices.</p> <p>Now focusing on the derivative of the output layer, for the case where \\(y_j\\) is non-zero, we have:</p> \\[ \\begin{align*} \\frac{\\partial y_{j}}{\\partial w_{k i}} &amp;= \\frac{\\partial}{\\partial w_{k i}}    \\sum_{i'}\\sum_{k'} w_{k' j}\\,w_{k i'}\\,x_{i'} \\\\[1ex] &amp;= \\sum_{i'} x_{i'}\\,    \\sum_{k'} \\frac{\\partial}{\\partial w_{k i}}    \\bigl(w_{k' j}\\,w_{k i'}\\bigr) \\\\[1ex] &amp;= \\sum_{i'} x_{i'} \\sum_{k'}    \\begin{cases}      2\\,w_{k i'}            &amp; k'=k \\wedge i'=j=i,\\\\      w_{k i'}              &amp; k'=k \\wedge (i' \\ne j \\wedge i' = i),\\\\      0                     &amp; \\text{otherwise}    \\end{cases} \\\\[1ex] &amp;= \\sum_{i'} x_{i'}\\,w_{k i'} \\;+\\; x_{j}\\,w_{k j} \\\\[1ex] &amp;= h_{k} \\;+\\; x_{j}\\,w_{k j}. \\end{align*} \\] <p>Let's do an intuition check on this derivative. The weight \\(w_{k i}\\) appears in two places: once in the hidden layer as \\(x_i\\)'s contribution to \\(h_k\\), and once in the output layer as \\(h_k\\)'s contribution to \\(y_j\\). So increasing \\(w_{k i}\\) will increase the output proportionally  to the value of \\(h_k\\), but then we need to add in the fact that \\(h_k\\) itself is also increased proportional to both the \\(i\\)th input and the current value of the weight. So our calculation seems intuitively correct.</p>"},{"location":"blog/2025/05/14/demonstrating-superposition/#computing-the-gradient-in-c","title":"Computing the Gradient in C","text":"<p>To compute the gradient in C,  we implement a gradient function that adds to a gradient accumulator:</p> <pre><code>float gradient(const params_t * p, const float x[N], float alpha, params_t * grad);\n</code></pre> <p>The simplest way is just to take the forward pass and keep track of temporary variables that appear in the gradient expression above and then add them together as prescribed. For example the hidden layer computation now looks like:</p> <pre><code>for (int m = 0; m &lt; M; m++) {\n    for (int n = 0; n &lt; N; n++) {\n        wkj_xj[m][n] = p-&gt;W[m][n] * x[n];\n        hk[m] += wkj_xj[m][n];\n    }\n}\n</code></pre> <p>And so on (1) as we compute the gradient, add to the accumulator, and return the loss.</p> <ol> <li><pre><code>float gradient(const params_t * p, const float x[N], float alpha, params_t * grad) {\n    // unlike the forward pass, we keep track of intermediate\n    // values that appear in the gradient\n    // our toy model is so small that all this fits comfortably \n    // in the thread stack\n    // alpha = L1 regularization coefficient\n    // grad is a pointer to the gradient accumulator\n    // returns loss\n    float wkj_xj[M][N];\n    float hk[M];\n    float y[N];\n    float delta[N];\n    float dL_wkj[M][N];\n    memset(wkj_xj, 0, sizeof(wkj_xj));\n    memset(hk, 0, sizeof(hk));\n    memset(y, 0, sizeof(y));\n    memset(delta, 0, sizeof(delta));\n    memset(dL_wkj, 0, sizeof(dL_wkj));\n    // hidden layer\n    for (int m = 0; m &lt; M; m++) {\n        for (int n = 0; n &lt; N; n++) {\n            wkj_xj[m][n] = p-&gt;W[m][n] * x[n];\n            hk[m] += wkj_xj[m][n];\n        }\n    }\n    // output layer\n    for (int n = 0; n &lt; N; n++) {\n        for (int m = 0; m &lt; M; m++) {\n            y[n] += p-&gt;W[m][n] * hk[m];\n        }\n        y[n] += p-&gt;b[n];\n        // ReLU activation\n        y[n] = y[n] &gt; 0 ? y[n] : 0;\n        // compute delta\n        delta[n] = y[n] - x[n];\n    }\n    // compute error\n    float L = 0;\n    for (int n = 0; n &lt; N; n++) {\n        float Ij = importance(n);\n        L += Ij * delta[n] * delta[n];\n    }\n    for (int n = 0; n &lt; N; n++) {\n        for (int m = 0; m &lt; M; m++) {\n            L += alpha * fabs(p-&gt;W[m][n]);\n        }\n        L += alpha * fabs(p-&gt;b[n]);\n    }\n    L /= 2;\n    for (int m = 0; m &lt; M; m++) {\n        for (int n = 0; n &lt; N; n++) {\n            if (y[n] &lt;= 0) continue;\n            dL_wkj[m][n] = importance(n) * delta[n] * (hk[m] + wkj_xj[m][n])\n                + alpha * (p-&gt;W[m][n] &gt; 0 ? 1 : -1);\n        }\n    }\n    // add to gradient accumulator\n    for (int m = 0; m &lt; M; m++) {\n        for (int n = 0; n &lt; N; n++) {\n            grad-&gt;W[m][n] -= dL_wkj[m][n];\n        }\n    }\n    for (int n = 0; n &lt; N; n++) {\n        if (y[n] &lt;= 0) continue;\n        grad-&gt;b[n] -= delta[n] + alpha * (p-&gt;b[n] &gt; 0 ? 1 : -1);\n    }\n    return L;\n}\n</code></pre></li> </ol>"},{"location":"blog/2025/05/14/demonstrating-superposition/#the-training-loop","title":"The Training Loop","text":"<p>Now we put it all together (1), adding in a random batch of size 1024 which provides some stochasticity to the gradient descent. Note that I'm not using any optimizer, and I've got regularization turned off.</p> <ol> <li><pre><code>params_t p;\nmemset(&amp;p, 0, sizeof(p));\n// initialize with random weights and biases\nfor (int j = 0; j &lt; N; j++) {\n    for (int k = 0; k &lt; M; k++) {\n        p.W[k][j] = frand() * 0.001;\n    }\n    p.b[j] = frand() * 0.001;\n}\nparams_t grad;\nfor (int r = 0; r &lt; runs; r++) {\n    memset(&amp;grad, 0, sizeof(grad));\n    float L = 0;\n    long batch[batch_size];\n    batch_indices(batch_size, batch);\n    for (long c = 0; c &lt; batch_size; c++) {\n        L += gradient(&amp;p, X[batch[c]], alpha, &amp;grad);\n    }\n    update(&amp;p, &amp;grad, eta / batch_size);\n    printf(\"run: %d\\n\", r);\n    printf(\"L: %1.04f\\n\", L / batch_size);\n    if (r % 100 == 99) {\n        // print b\n        printmat(\"b\", 1, N, p.b);\n        // print W\n        printmat(\"W\", M, N, p.W);\n        // print grad w and b\n        printmat(\"grad w\", M, N, grad.W);\n        printmat(\"grad b\", 1, N, grad.b);\n    }\n    fflush(stdout);\n}\n</code></pre></li> </ol> <p>This C program outputs a long log of the weights and loss during the training run. Runs take about 10 seconds for 10000 batches, which is enough to fully converge.</p> <p>It took only a little bit of trial and error to get the learning rate right.</p> <p>I then asked <code>o3</code> (1) to take the outputted log and make an animation, resulting in the video shared at the top. Here was the prompt that got me 90% of the way to a working animation:</p> <ol> <li>One of the hardest parts of learning to use LLMs I find is knowing when and when not to use them. For building visualizations, I find LLMs incredibly helpful, while for learning, it's best to battle through the details oneself.</li> </ol> <p>Please write a Python program that takes a file called log with this format: (pasted example log snippet) and uses matplotlib to render loss as a function of the run, and the W matrix showing how each unit input vector is mapped to the hidden dimensions (2d) which should be a plot with one scatter dot for each of the 5 input unit vectors. Make this an animation showing how the points migrated over time, keeping the xy limits fixed so it is stable. Include a moving average of the loss as a line plot.</p>"},{"location":"blog/2025/05/14/demonstrating-superposition/#intiution-about-local-minima","title":"Intiution about Local Minima","text":"<p>I'll close with an animation of the same model but with \\(n=32\\) features, and importance decaying as \\(0.9^i\\). Notice how it converges to a suboptimal solution!</p>    Your browser does not support the video tag.  <p>You can see that the pentagon quickly forms out of features 0 through 4, and then features 5, 6, and 7 start to emerge, but then it is the less important feature 7 which pushes its way out to form a stable hexagon.  Why is that? It seems to be because features 5 and 6  were unlucky enough to be on the side of the pentagon shared with higher-importance features while 7 had the good fortune of being near the relatively weaker feature 4 which it could push out of the way.</p> <p>Bottom line: there are non-trivial local minima even in simple models and we can actually have some hope of gaining intuition about them.</p>"},{"location":"embedded/","title":"Blog","text":""},{"location":"research/charformers/","title":"Character-based Transformers","text":"<p>Way back when I had fun with little n-gram models, that I fondly recall could write pseudo-Tolkien when training it on a text copy of The Hobbit. It was fun to watch how as n increased from 3 to 5 the words became more and more coherently spelled. You now understand my sadness at the tokenization step used in modern LLMs: it strips them of the  very first learning step of discovering the morphemes, smallest units of meaning.</p> <p>So I'd like to experiment with some character-based transformers. I have no expectation that I'd be able to get coherent English text out of them without massive GPU resources, but perhaps we can intepret some patterns of morphological learning?</p>"},{"location":"research/charformers/#sanitizing-the-data","title":"Sanitizing the Data","text":"<p>We are concerned with English text, and we want to keep  the language as small as possible, so let's just work with lowercase letters plus space and a few punctuation characters:</p> <p><code>abcdefghijklmnopqrstuvwxyz .,?'\\n</code></p> <p>Note that we've merged the apostrophe, single, and double quotes, we keep distinct period (but convert exclaimation to period), a distinct question mark, and all other punctuation will be replaced with spaces. Newline has its own symbol, which represents a paragraph break or could be used as a stop token.</p> <p>For my purposes I'll feed in a text copy of The Lord of the Rings, because I know this book so well that it should help me interpret the small model gibberish I'm sure to get when sampling generatively.</p>"},{"location":"research/charformers/#n-grams-revisited","title":"N-Grams Revisited","text":"<p>I like to start with the simplest possible model, which within the neural network paradigm would be a single-layer network. (1)</p> <ol> <li>If we want to be pedantic, it would be a zero-layer network with just a bias term predicting just based on character frequency.</li> </ol>"},{"location":"research/threedigit-notes/","title":"Three Digit Math","text":"<p>I devoured Heinlein's Stranger in a Strange Land in high school, so naturally I was excited to hear about neural nets \"grokking\" three digit addition. I recall seeing a graph of training and test accuracy over time, where the training accuracy reaches 100% early on, but with test accuracy at chance---no generalization---but then after a long period of stasis, the test accuracy quite suddenly shoots up to 100% also. The network \"grokked\" the problem.</p> <p>So I ask, what's the simplest model I can build that can grok three digit addition? And, can we look inside and see how it does it?</p> <p>Now, I could go and read the paper, but first I'm just going to play around myself.</p> <p>\"Research is what I'm doing when I don't know what I'm doing.\" - Wernher von Braun</p>"},{"location":"research/threedigit-notes/#dataset","title":"Dataset","text":"<p>Our dataset will be all 3-digit addition equations that have 3-digit answers, such as</p> \\[ 123 + 456 = 579. \\] <p>I'll simply enumerate all the equations:</p> <pre><code>def all_equations():\n    for i in range(100, 1000):\n        for j in range(100, 1000):\n            k = i + j\n            if k &gt; 999:\n                break\n            yield f\"{i}+{j}={k}\"\n</code></pre> <p>then shuffle, and split into 90% training and 10% test.</p>"},{"location":"research/threedigit-notes/#embeddings","title":"Embeddings","text":"<p>Now, we need to decide how to encode the digits as vectors. The simplest approach is to use one-hot encoding, where each digit is represented as a vector of length 10 with just a single dimension set to 1, the rest set to 0. So our NN will be learning a mapping:</p> \\[ \\mathbb{R}^{6 \\times 10} \\rightarrow \\mathbb{R}^{3 \\times 10} \\] <p>For the example above, the input is</p> \\[ X = \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix}, \\] <p>and the correct output is</p> \\[ Y = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}. \\] <p>To the network, the input is a 60-dimensional vector, I've just formatted it as a 6x10 matrix for clarity.</p>"},{"location":"research/threedigit-notes/#a-first-dumb-model","title":"A First Dumb Model","text":"<p>Let's start with the simplest possible model, one without any hidden layers:</p> <pre><code>graph LR\n    subgraph Inputs\n        I[6 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;60 values]\n    end\n\n    subgraph Output Layer\n        O[3 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;30 neurons]\n    end\n\n    I --&gt; O</code></pre>"},{"location":"research/threedigit-notes/#the-math","title":"the math","text":"<p>Let's look at the math for this model. We have a linear transformation of the input:</p> \\[ Z = W X + b \\] <p>where:</p> <ul> <li>\\(X \\in \\mathbb{R}^{60}\\) is the input vector</li> <li>\\(W \\in \\mathbb{R}^{30 \\times 60}\\) is the weight matrix</li> <li>\\(b \\in \\mathbb{R}^{30}\\) is the bias vector</li> <li>\\(Z \\in \\mathbb{R}^{30}\\) is the pre-activation output</li> </ul> <p>Then applying softmax to get the final output for each digit position \\(d \\in \\{0, 1, 2\\}\\) and each digit class \\(c \\in \\{0, 1, ..., 9\\}\\):</p> \\[ Y_{d,c} = \\frac{e^{Z_{d,c}}}{\\sum_{c'=0}^{9} e^{Z_{d,c'}}} \\] <p>This gives us probability distributions over the 10 possible values for each of the 3 output digits.</p> <p>Our little \"simplest model\" already has 1830 parameters!</p>"},{"location":"research/threedigit-notes/#lets-fire-up-an-autograd","title":"let's fire up an autograd?","text":"<p>To be continued...</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/fractal-u/","title":"fractal-u","text":""},{"location":"blog/category/tmos/","title":"tmos","text":""}]}