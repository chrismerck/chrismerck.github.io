{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"welcome to my digital lab notebook","text":""},{"location":"#about-me","title":"about me","text":"<p>I'm an entrepreneur-engineer, researcher, engineer, technologist, musician, and linguaphile. </p> <p>I am co-founder and CTO at Olibra (Bond Home), where I lead a team building next-generation user experiences for smart home.</p> <p>My primary research interest at this time is AI interpretability, specifically how character-level language models learn morphemes and their meanings.</p> <p>Otherwise I spend my time covering sad girl music, taking friends foraging, ever so gradually learning Latin, and teaching kids to solder and code.</p>"},{"location":"#research","title":"research","text":""},{"location":"#three-digit-math","title":"Three Digit Math","text":"<p>I'm exploring how neural networks \"grok\" mathematical operations in my Three Digit Math project, investigating the phenomenon where models suddenly generalize after extended training periods.</p>"},{"location":"research/threedigit-notes/","title":"Three Digit Math","text":"<p>I devoured Heinlein's Stranger in a Strange Land in high school, so naturally I was excited to hear about neural nets \"grokking\" three digit addition. I recall seeing a graph of training and test accuracy over time, where the training accuracy reaches 100% early on, but with test accuracy at chance---no generalization---but then after a long period of stasis, the test accuracy quite suddenly shoots up to 100% also. The network \"grokked\" the problem.</p> <p>So I ask, what's the simplest model I can build that can grok three digit addition? And, can we look inside and see how it does it?</p> <p>Now, I could go and read the paper, but first I'm just going to play around myself.</p> <p>\"Research is what I'm doing when I don't know what I'm doing.\" - Wernher von Braun</p>"},{"location":"research/threedigit-notes/#dataset","title":"Dataset","text":"<p>Our dataset will be all 3-digit addition equations that have 3-digit answers, such as</p> \\[ 123 + 456 = 579. \\] <p>I'll simply enumerate all the equations:</p> <pre><code>def all_equations():\n    for i in range(100, 1000):\n        for j in range(100, 1000):\n            k = i + j\n            if k &gt; 999:\n                break\n            yield f\"{i}+{j}={k}\"\n</code></pre> <p>then shuffle, and split into 90% training and 10% test.</p>"},{"location":"research/threedigit-notes/#embeddings","title":"Embeddings","text":"<p>Now, we need to decide how to encode the digits as vectors. The simplest approach is to use one-hot encoding, where each digit is represented as a vector of length 10 with just a single dimension set to 1, the rest set to 0. So our NN will be learning a mapping:</p> \\[ \\mathbb{R}^{6 \\times 10} \\rightarrow \\mathbb{R}^{3 \\times 10} \\] <p>For the example above, the input is</p> \\[ X = \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix}, \\] <p>and the correct output is</p> \\[ Y = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}. \\] <p>To the network, the input is a 60-dimensional vector, I've just formatted it as a 6x10 matrix for clarity.</p>"},{"location":"research/threedigit-notes/#a-first-dumb-model","title":"A First Dumb Model","text":"<p>Let's start with the simplest possible model, one without any hidden layers:</p> <pre><code>graph LR\n    subgraph Inputs\n        I[6 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;60 values]\n    end\n\n    subgraph Output Layer\n        O[3 digits&lt;br/&gt;One-hot encoded&lt;br/&gt;&lt;br/&gt;30 neurons]\n    end\n\n    I --&gt; O</code></pre>"},{"location":"research/threedigit-notes/#the-math","title":"the math","text":"<p>Let's look at the math for this model. We have a linear transformation of the input:</p> \\[ Z = W X + b \\] <p>where:</p> <ul> <li>\\(X \\in \\mathbb{R}^{60}\\) is the input vector</li> <li>\\(W \\in \\mathbb{R}^{30 \\times 60}\\) is the weight matrix</li> <li>\\(b \\in \\mathbb{R}^{30}\\) is the bias vector</li> <li>\\(Z \\in \\mathbb{R}^{30}\\) is the pre-activation output</li> </ul> <p>Then applying softmax to get the final output for each digit position \\(d \\in \\{0, 1, 2\\}\\) and each digit class \\(c \\in \\{0, 1, ..., 9\\}\\):</p> \\[ Y_{d,c} = \\frac{e^{Z_{d,c}}}{\\sum_{c'=0}^{9} e^{Z_{d,c'}}} \\] <p>This gives us probability distributions over the 10 possible values for each of the 3 output digits.</p> <p>Our little \"simplest model\" already has 1830 parameters!</p>"},{"location":"research/threedigit-notes/#lets-fire-up-tinygrad","title":"let's fire up tinygrad","text":"<p>So if we visualize the model, we've got a data flow like this:</p> <pre><code>graph LR\n    subgraph Input\n        X_mat[\"X (6x10)&lt;br/&gt;One-hot Digits\"]\n    end\n\n    subgraph Flatten\n        X_vec[\"X (60)\"]\n    end\n\n    subgraph Linear Layer\n        W[\"W (30x60)\"]\n        b[\"b (30)\"]\n        Z_vec[\"Z = WX + b&lt;br/&gt;(30)\"]\n    end\n\n    subgraph Reshape &amp; Softmax\n        Z_mat[\"Z (3x10)\"]\n        Y_mat[\"Y (3x10)&lt;br/&gt;Softmax(Z, axis=1)\"]\n    end\n\n    X_mat --&gt;|\"Flatten\"| X_vec\n    X_vec --&gt; Z_vec\n    W --&gt; Z_vec\n    b --&gt; Z_vec\n    Z_vec --&gt;|\"Reshape\"| Z_mat\n    Z_mat --&gt;|\"Softmax&lt;br/&gt;per row\"| Y_mat</code></pre> <p>To be continued...</p>"}]}